{"paragraphs":[{"text":"%md\n\n# Streaming machine learning","user":"anonymous","dateUpdated":"2017-11-08T08:24:38+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Streaming machine learning</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1510129463783_-900869882","id":"20171108-082423_839563490","dateCreated":"2017-11-08T08:24:23+0000","dateStarted":"2017-11-08T08:24:38+0000","dateFinished":"2017-11-08T08:24:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3625"},{"text":"%md\n\n## Put your ip below","user":"anonymous","dateUpdated":"2017-11-08T08:42:56+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Put your ip below</h2>\n</div>"}]},"apps":[],"jobName":"paragraph_1509717905802_1910546186","id":"20171103-140505_1790236293","dateCreated":"2017-11-03T14:05:05+0000","dateStarted":"2017-11-08T08:24:44+0000","dateFinished":"2017-11-08T08:24:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3626"},{"text":"val myIP = \"10.28.28.236\"\n","user":"anonymous","dateUpdated":"2017-11-07T13:02:55+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nmyIP: String = 10.28.28.236\n"}]},"apps":[],"jobName":"paragraph_1509717923091_-532594006","id":"20171103-140523_635139247","dateCreated":"2017-11-03T14:05:23+0000","dateStarted":"2017-11-07T13:02:55+0000","dateFinished":"2017-11-07T13:02:55+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3627"},{"text":"%md\n\n## Let's create a Streaming Context\n","user":"anonymous","dateUpdated":"2017-11-08T08:25:40+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Let&rsquo;s create a Streaming Context</h2>\n</div>"}]},"apps":[],"jobName":"paragraph_1510129487786_63172944","id":"20171108-082447_128463663","dateCreated":"2017-11-08T08:24:47+0000","dateStarted":"2017-11-08T08:25:40+0000","dateFinished":"2017-11-08T08:25:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3628"},{"text":"import org.apache.spark._\nimport org.apache.spark.streaming._\nimport org.apache.spark.streaming.StreamingContext._\n\n// This streaming context will be doing 5 seconds batches\nval ssc = new StreamingContext(sc, Seconds(5))","user":"anonymous","dateUpdated":"2017-11-08T08:25:29+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.spark._\n\nimport org.apache.spark.streaming._\n\nimport org.apache.spark.streaming.StreamingContext._\n\nssc: org.apache.spark.streaming.StreamingContext = org.apache.spark.streaming.StreamingContext@3512809d\n"}]},"apps":[],"jobName":"paragraph_1509715192498_-1084829626","id":"20171103-131952_1212526556","dateCreated":"2017-11-03T13:19:52+0000","dateStarted":"2017-11-07T13:02:55+0000","dateFinished":"2017-11-07T13:02:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3629"},{"text":"%md\n\n## Let's consume the kafka streams£","user":"anonymous","dateUpdated":"2017-11-08T08:26:01+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Let&rsquo;s consume the kafka streams</h2>\n</div>"}]},"apps":[],"jobName":"paragraph_1510129502811_-1705887175","id":"20171108-082502_1709125459","dateCreated":"2017-11-08T08:25:02+0000","dateStarted":"2017-11-08T08:26:01+0000","dateFinished":"2017-11-08T08:26:01+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3630"},{"text":"import org.apache.kafka.clients.consumer.ConsumerRecord\nimport org.apache.kafka.common.serialization.StringDeserializer\nimport org.apache.spark.streaming.kafka010._\nimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent\nimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe\n\n// Kafka parameters\nval kafkaParams = Map(\n    \"bootstrap.servers\" -> s\"$myIP:9092\",\n    \"value.deserializer\" ->classOf[StringDeserializer],\n    \"key.deserializer\" -> classOf[StringDeserializer],\n    \"auto.offset.reset\" -> \"latest\",\n    \"enable.auto.commit\" -> (false: java.lang.Boolean)\n    )\n    \n// Consume the \"train\" topic  \nval trainInputStream = KafkaUtils.createDirectStream[String, String](\n  ssc,\n  PreferConsistent,\n  Subscribe[String, String](Array(\"train\"), kafkaParams + (\"group.id\" -> \"group1\"))\n)\n\n// Consume the \"test\" topic\nval testInputStream = KafkaUtils.createDirectStream[String, String](\n  ssc,\n  PreferConsistent,\n  Subscribe[String, String](Array(\"test\"), kafkaParams + (\"group.id\" -> \"group2\"))\n)\n\n// Consume the \"predict\" topic\nval predictInputStream = KafkaUtils.createDirectStream[String, String](\n  ssc,\n  PreferConsistent,\n  Subscribe[String, String](Array(\"predict\"), kafkaParams + (\"group.id\" -> \"group3\"))\n)\n","user":"anonymous","dateUpdated":"2017-11-08T08:27:22+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.kafka.clients.consumer.ConsumerRecord\n\nimport org.apache.kafka.common.serialization.StringDeserializer\n\nimport org.apache.spark.streaming.kafka010._\n\nimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent\n\nimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe\n\nkafkaParams: scala.collection.immutable.Map[String,java.io.Serializable] = Map(key.deserializer -> class org.apache.kafka.common.serialization.StringDeserializer, auto.offset.reset -> latest, bootstrap.servers -> 10.28.28.236:9092, enable.auto.commit -> false, value.deserializer -> class org.apache.kafka.common.serialization.StringDeserializer)\n\nres85: (String, String) = (Map(key.deserializer -> class org.apache.kafka.common.serialization.StringDeserializer, auto.offset.reset -> latest, bootstrap.servers -> 10.28.28.236:9092, enable.auto.commit -> false, value.deserializer -> class org.apache.kafka.common.serialization.StringDeserializer)group.id,group1)\n\ntrainInputStream: org.apache.spark.streaming.dstream.InputDStream[org.apache.kafka.clients.consumer.ConsumerRecord[String,String]] = org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@7c73c0ff\n\ntestInputStream: org.apache.spark.streaming.dstream.InputDStream[org.apache.kafka.clients.consumer.ConsumerRecord[String,String]] = org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@1c8cb8e9\n\npredictInputStream: org.apache.spark.streaming.dstream.InputDStream[org.apache.kafka.clients.consumer.ConsumerRecord[String,String]] = org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@397c2ee\n"}]},"apps":[],"jobName":"paragraph_1509717626159_2140999265","id":"20171103-140026_1709125459","dateCreated":"2017-11-03T14:00:26+0000","dateStarted":"2017-11-07T13:02:55+0000","dateFinished":"2017-11-07T13:03:00+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3631"},{"text":"%md\n\n## Let's do some pre-processing on the streams for them to be usable by MLLib\n\n* Each (k:String, v:String) coming from kafka represents a 2D point (x,y)\n* Each message also has meta-data attached that uniquely identifies a message (topic, partition, offset)\n* In the end, we create a Key-Value pair where the Key identifies the message and the value is a [LabeledPoint](http://spark.apache.org/docs/2.1.0/api/scala/index.html#org.apache.spark.mllib.regression.LabeledPoint)\n","user":"anonymous","dateUpdated":"2017-11-08T08:32:37+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Let&rsquo;s do some pre-processing on the streams for them to be usable by MLLib</h2>\n<ul>\n  <li>Each (k:String, v:String) coming from kafka represents a 2D point (x,y)</li>\n  <li>Each message also has meta-data attached that uniquely identifies a message (topic, partition, offset)</li>\n  <li>In the end, we create a Key-Value pair where the Key identifies the message and the value is a <a href=\"http://spark.apache.org/docs/2.1.0/api/scala/index.html#org.apache.spark.mllib.regression.LabeledPoint\">LabeledPoint</a></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1510129690126_-1239127107","id":"20171108-082810_172866301","dateCreated":"2017-11-08T08:28:10+0000","dateStarted":"2017-11-08T08:32:37+0000","dateFinished":"2017-11-08T08:32:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3632"},{"text":"import org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.regression.LabeledPoint\n\n//Transform the train stream to k,v\nval trainStream = trainInputStream.map(record => \n    (\n        (record.topic, record.partition, record.offset),\n        LabeledPoint(record.key.toDouble, Vectors.dense(record.value.toDouble))\n    )\n)\n//Transform the test stream \nval testStream = testInputStream.map(record => \n    (\n        (record.topic, record.partition, record.offset),\n        LabeledPoint(record.key.toDouble, Vectors.dense(record.value.toDouble))\n    )\n)\n//Transform the predict stream. Put a dummy label since we want predictions !\nval predictStream = predictInputStream.map(record => \n    (\n        (record.topic, record.partition, record.offset),\n        LabeledPoint( 0.0 , Vectors.dense(record.value.toDouble))\n    )\n)\n","user":"anonymous","dateUpdated":"2017-11-08T08:35:39+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.spark.mllib.linalg.Vectors\n\nimport org.apache.spark.mllib.regression.LabeledPoint\n\ntrainStream: org.apache.spark.streaming.dstream.DStream[((String, Int, Long), org.apache.spark.mllib.regression.LabeledPoint)] = org.apache.spark.streaming.dstream.MappedDStream@568cd5c5\n\ntestStream: org.apache.spark.streaming.dstream.DStream[((String, Int, Long), org.apache.spark.mllib.regression.LabeledPoint)] = org.apache.spark.streaming.dstream.MappedDStream@38d4162a\n\npredictStream: org.apache.spark.streaming.dstream.DStream[((String, Int, Long), org.apache.spark.mllib.regression.LabeledPoint)] = org.apache.spark.streaming.dstream.MappedDStream@1aa2346d\n"}]},"apps":[],"jobName":"paragraph_1509718537996_-1872834245","id":"20171103-141537_549291714","dateCreated":"2017-11-03T14:15:37+0000","dateStarted":"2017-11-07T13:02:56+0000","dateFinished":"2017-11-07T13:03:01+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3633"},{"text":"%md\n\n## We are now ready to build the model","user":"anonymous","dateUpdated":"2017-11-08T08:35:58+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510130142502_-31954745","id":"20171108-083542_1870300499","dateCreated":"2017-11-08T08:35:42+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:3634"},{"text":"import org.apache.spark.mllib.regression.StreamingLinearRegressionWithSGD\n\n// Init the model\nval slaSGD = new StreamingLinearRegressionWithSGD()\n    .setInitialWeights( Vectors.dense(0.0))\n    .setStepSize(.0001)\nslaSGD.algorithm.setIntercept(true)\n\n// Set up the stream it trains on  \nslaSGD.trainOn(trainStream.map(_._2))\n\n//Join all the streams together for predictions\nval allStreams = trainStream\n    .union(testStream)\n    .union(predictStream)\n\n//This will output a Key-Value where the key still uniquely identifies an input message and the value is the prediction\nval predictions = slaSGD.predictOnValues(allStreams.mapValues(_.features))\n\n//Join back with the initial data of the message.\n//We now have the the labeled point along with its prediction\nval predAndReal = allStreams.join(predictions)\n\n//Transform each Key-Value to a simple Map structure (can be easily translated to a JSON for Elasticsearch)\nval toESStream = predAndReal.map{\n    case ((topic, partition, offset), (point, prediction)) =>\n        Map(\n            \"topic\"-> topic,\n            \"partition\" -> partition,\n            \"offset\" -> offset,\n            \"y\" -> point.features(0),\n            \"x\" -> point.label,\n            \"pred\" -> prediction\n        )\n}","user":"anonymous","dateUpdated":"2017-11-08T08:42:01+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.spark.mllib.regression.StreamingLinearRegressionWithSGD\n\nslaSGD: org.apache.spark.mllib.regression.StreamingLinearRegressionWithSGD = org.apache.spark.mllib.regression.StreamingLinearRegressionWithSGD@7ee2d5f\n\nres102: slaSGD.algorithm.type = org.apache.spark.mllib.regression.LinearRegressionWithSGD@7bde31f\n\nallStreams: org.apache.spark.streaming.dstream.DStream[((String, Int, Long), org.apache.spark.mllib.regression.LabeledPoint)] = org.apache.spark.streaming.dstream.UnionDStream@663b12b8\n\npredictions: org.apache.spark.streaming.dstream.DStream[((String, Int, Long), Double)] = org.apache.spark.streaming.dstream.MapValuedDStream@73d69dcb\n\npredAndReal: org.apache.spark.streaming.dstream.DStream[((String, Int, Long), (org.apache.spark.mllib.regression.LabeledPoint, Double))] = org.apache.spark.streaming.dstream.TransformedDStream@6c8bc945\n\ntoESStream: org.apache.spark.streaming.dstream.DStream[scala.collection.immutable.Map[String,Any]] = org.apache.spark.streaming.dstream.MappedDStream@6ce07de8\n"}]},"apps":[],"jobName":"paragraph_1509715339060_-1043429987","id":"20171103-132219_752317564","dateCreated":"2017-11-03T13:22:19+0000","dateStarted":"2017-11-07T13:03:00+0000","dateFinished":"2017-11-07T13:03:03+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3635"},{"text":"import org.elasticsearch.spark.streaming._\n\n//Save the predictions\ntoESStream.saveToEs(\"spark/docs\",  Map(\"es.nodes\" -> \"elk\", \"es.port\" -> \"9200\"))\n\n","user":"anonymous","dateUpdated":"2017-11-08T08:39:49+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.elasticsearch.spark.streaming._\n"}]},"apps":[],"jobName":"paragraph_1509715691349_-1799575947","id":"20171103-132811_1900830255","dateCreated":"2017-11-03T13:28:11+0000","dateStarted":"2017-11-07T13:03:01+0000","dateFinished":"2017-11-07T13:03:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3636"},{"text":"//Start the stream\nssc.start()\n\n","user":"anonymous","dateUpdated":"2017-11-08T08:39:57+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1509722515215_1234698302","id":"20171103-152155_498430584","dateCreated":"2017-11-03T15:21:55+0000","dateStarted":"2017-11-07T13:03:04+0000","dateFinished":"2017-11-07T13:03:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3637"},{"text":"%md\n\n## Note:\n\n### This last cell can be un-commented in order to stop the streaming context (and all streams built on top of it) without stopping the underlying SparkContext, that way the whole notebook can be re-run","user":"anonymous","dateUpdated":"2017-11-08T08:41:26+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Note:</h2>\n<h3>This last cell can be un-commented in order to stop the streaming context (and all streams built on top of it) without stopping the underlying SparkContext, that way the whole notebook can be re-run</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1510130397974_2047965516","id":"20171108-083957_1447233452","dateCreated":"2017-11-08T08:39:57+0000","dateStarted":"2017-11-08T08:41:26+0000","dateFinished":"2017-11-08T08:41:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3638"},{"text":"//ssc.stop(false)","user":"anonymous","dateUpdated":"2017-11-07T13:02:55+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1509722765532_-240107174","id":"20171103-152605_312515462","dateCreated":"2017-11-03T15:26:05+0000","dateStarted":"2017-11-07T13:03:04+0000","dateFinished":"2017-11-07T13:03:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3639"},{"user":"anonymous","dateUpdated":"2017-11-07T13:02:55+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1509957746212_-957308691","id":"20171106-084226_1854730434","dateCreated":"2017-11-06T08:42:26+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:3640"}],"name":"Streaming ML -> ES","id":"2CZKG1XFR","angularObjects":{"2D16W9MXG:shared_process":[],"2CWTVRSBZ:shared_process":[],"2CY52ARMF:shared_process":[],"2CZ1BRDVH:shared_process":[],"2CWDMYF35:shared_process":[],"2CY3ZUSFY:shared_process":[],"2CYQNJ25U:shared_process":[],"2CYZK8HE9:shared_process":[],"2CXGQN1MQ:shared_process":[],"2CWRYJYJM:shared_process":[],"2CY8CPYQZ:shared_process":[],"2CY6HFJCM:shared_process":[],"2CWCAB3WM:shared_process":[],"2CY6BYDBB:shared_process":[],"2CXVEH3HK:shared_process":[],"2CWETFSF1:shared_process":[],"2CYBWPP34:shared_process":[],"2CYNX62VR:shared_process":[],"2CZTYFY3W:shared_process":[],"2CXVWXXCC:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}